{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#End-to-End Pipeline for User 1 Statement Similarity Analysis"
      ],
      "metadata": {
        "id": "aijWuoc7zaeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I've created an end-to-end pipeline to find the similarity between the statement of User 1 and the statements of other users. I have utilized BERT and Word2Vec embeddings to compute similarity scores and for ranking."
      ],
      "metadata": {
        "id": "92psUluNFRei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Overview\n",
        "1. Import Libraries\n",
        "2. Load Models\n",
        "3. User Statements\n",
        "4. Preprocessing Functions\n",
        "5. Embedding Functions\n",
        "6. Cosine Similarity Function\n",
        "7. Computation of Embeddings\n",
        "8. Computation of Ranking based on similarity scores"
      ],
      "metadata": {
        "id": "gebMiv-zFiqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information used for matching:**\n",
        "\n",
        "• User 1: I think that fashion trends dictate what we wear and that beauty standards\n",
        "can influence our choices in makeup and skincare.\n",
        "• User 2: Finding the perfect outfit can be a daunting task but experimenting with\n",
        "different hairstyles can help explore different looks and add flair to your style.\n",
        "• User 3: Accessories can elevate any ensemble and adding bold colors to your look\n",
        "can be empowering and fun.\n",
        "• User 4: Fashion shows showcase the latest designs and styles and it’s a great place\n",
        "to invest in quality products.\n",
        "• User 5: Personal style reflects individuality and creativity, and I think that confidence\n",
        "is the best accessory someone can wear."
      ],
      "metadata": {
        "id": "kwbUEMjx0AaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Libraries**"
      ],
      "metadata": {
        "id": "2zMU3Sg50cZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install gensim\n",
        "# ! pip install transformers\n",
        "# ! pip install torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch"
      ],
      "metadata": {
        "id": "MimVBwNDz1Bo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above step, we have imported all the necessary modules\n",
        "\n",
        "1.   'numpy' for numerical computation part.\n",
        "2.   'BertTokenizer' and 'BertModel' from Hugging Face's Transformer library for creating the Bert embeddings in the vector space.\n",
        "\n",
        "3. 'gensim' for creating Word2Vec embeddings\n",
        "\n",
        "4. 'cosine_similarity' from sklearn for calculating the similary in between my vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "vjOB1O_G09qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Load Pre-trained**"
      ],
      "metadata": {
        "id": "BUgThRfm12x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "6xFbTM7I12Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, I have used the pretrained models Bert and word2vec.\n",
        "\n",
        "1. I have used \"bert-base-uncased\" as it's the base version of bert which can be suitable for my problem statement which is a general purpose task.\n",
        "2. For word2vec have used 'word2vec-google-news-300' which refers to a pre-trained model trained on a large dataset of Google News articles. It's good for capturing semantic relationship between the text."
      ],
      "metadata": {
        "id": "6m4IL7mQ2jge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Users Statements**"
      ],
      "metadata": {
        "id": "1f-gfdGr4j-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_statements = [\n",
        "    \"I think that fashion trends dictate what we wear and that beauty standards can influence our choices in makeup and skincare.\",\n",
        "    \"Finding the perfect outfit can be a daunting task but experimenting with different hairstyles can help explore different looks and add flair to your style.\",\n",
        "    \"Accessories can elevate any ensemble and adding bold colors to your look can be empowering and fun.\",\n",
        "    \"Fashion shows showcase the latest designs and styles and it’s a great place to invest in quality products.\",\n",
        "    \"Personal style reflects individuality and creativity, and I think that confidence is the best accessory someone can wear.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "EitLQWFa085l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Preprocessing Functions**"
      ],
      "metadata": {
        "id": "-yk-5bA44uCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_bert(statement):\n",
        "  inputs = tokenizer(statement, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "  return inputs\n",
        "\n",
        "def preprocess_word2vec(statement):\n",
        "  tokens = statement.lower().split()\n",
        "  return [word for word in tokens if word in word2vec_model]"
      ],
      "metadata": {
        "id": "5QiMBo5k05Ye"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above step will preprocess user's statements for Bert and word2vec model.\n",
        "1. 'preprocess_bert' function converts them into bert_tokens  \n",
        "2. 'preprocess_word2vec' function do lowercasing, tokenization and filters out word not present in word2vec vocabulary"
      ],
      "metadata": {
        "id": "W2CFfPG85czX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Embeddings Functions for Computation**"
      ],
      "metadata": {
        "id": "qAEYsPUz6lrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bert_embeddings(inputs):\n",
        "  with torch.no_grad():\n",
        "    outputs = bert_model(**inputs)\n",
        "  embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "  return embeddings.numpy()"
      ],
      "metadata": {
        "id": "VNNBvF8l5b1V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_word2vec_embeddings(statement):\n",
        "  return np.mean(word2vec_model[preprocess_word2vec(statement)], axis=0)"
      ],
      "metadata": {
        "id": "PcFN12BJ7FW-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above steps Functions computing Bert and word2vec embeddings for the input statements.\n",
        "1.  The compute_bert_embeddings function calculates the mean of the last hidden states of the BERT model.\n",
        "2.  The compute_word2vec_embeddings function computes the average Word2Vec embedding for the words in the statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "D4_PzVny7Tlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Compute Cosine Similarity Function**"
      ],
      "metadata": {
        "id": "vnUHJshF7sGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "  return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * (np.linalg.norm(vec2)))"
      ],
      "metadata": {
        "id": "wex3jFK17SuE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function will calculate the cosine similary between two vectors using numpy."
      ],
      "metadata": {
        "id": "EFTSwv_t8CBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Preprocessing of User 1's statement and computation embeddings**"
      ],
      "metadata": {
        "id": "d72RhQJQ9ooS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user 1's statement for Bert\n",
        "user1_bert_inputs = preprocess_bert(user_statements[0])\n",
        "\n",
        "#computation of bert embeddings for user 1\n",
        "user1_bert_embeddings = compute_bert_embeddings(user1_bert_inputs)\n",
        "# print('user1_bert_embeddings', user1_bert_embeddings[0])\n",
        "\n",
        "#compuation of word2vec embeddings for user 1's statement\n",
        "user1_word2vec_embeddings = compute_word2vec_embeddings(user_statements[0])\n",
        "# print('user1_word2vec_embeddings', user1_word2vec_embeddings[0])"
      ],
      "metadata": {
        "id": "SgnBYIdU8BOU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above step conversion of text into embedding is done for both word2vec and bert"
      ],
      "metadata": {
        "id": "7XHuVYHr-0_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Computation of similarity scores for User1 with other users**"
      ],
      "metadata": {
        "id": "yTeUSsKu_JMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing list for storing similarity scores\n",
        "bert_similarities =[]\n",
        "word2vec_similarities = []\n",
        "\n",
        "#iterate over user's other than user1\n",
        "for statement in user_statements[1:]:\n",
        "  #preprocess for bert\n",
        "  statement_bert_inputs = preprocess_bert(statement)\n",
        "  #compute bert embeddings for the other user's statement\n",
        "  statement_bert_embeddings = compute_bert_embeddings(statement_bert_inputs)\n",
        "\n",
        "  #compute Word2Vec embeddings for the statement\n",
        "  statement_word2vec_embeddings = compute_word2vec_embeddings(statement)\n",
        "\n",
        "  #calculation of cosine similarity between user 1 and the statements of others\n",
        "  bert_similarity = cosine_similarity(user1_bert_embeddings, statement_bert_embeddings)\n",
        "  word2vec_similarity = cosine_similarity(user1_word2vec_embeddings, statement_word2vec_embeddings)\n",
        "\n",
        "  # Append similarity scores to both lists\n",
        "  bert_similarities.append(bert_similarity)\n",
        "  word2vec_similarities.append(word2vec_similarity)"
      ],
      "metadata": {
        "id": "-qxmAn479d2Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above steps have following:\n",
        "1. preprocess the statement of user1 using preprocess function to get input for Bert.\n",
        "2. computed the Bert and word2vec embeddings for user1\n",
        "3. computed embeddings for other user's statements also\n",
        "4. calculated cosine similarity scores and stored in each list 'bert_similarities' and 'word2vec_similarities'"
      ],
      "metadata": {
        "id": "JEbQHbgiBHk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9: Ranking based on similarity scores**"
      ],
      "metadata": {
        "id": "JWFgLaCGB27P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices_bert = np.argsort(bert_similarities)[::-1]\n",
        "sorted_indices_word2vec = np.argsort(word2vec_similarities)[::-1]"
      ],
      "metadata": {
        "id": "ZFP-gjtyA5CA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank's Output with User1 using Bert embeddings**"
      ],
      "metadata": {
        "id": "wZZbQ7GwCFat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Rank matches with user1 using Bert embeddings:')\n",
        "for i, ix in enumerate(sorted_indices_bert, 1):\n",
        "  print(f'Rank {i}: User {ix +2} (Similarity Score: {bert_similarities[ix]})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-u8XAQ6CEts",
        "outputId": "cbc95956-8303-4760-8111-8c55c25ae592"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank matches with user1 using Bert embeddings:\n",
            "Rank 1: User 5 (Similarity Score: 0.8226811289787292)\n",
            "Rank 2: User 4 (Similarity Score: 0.7725504040718079)\n",
            "Rank 3: User 3 (Similarity Score: 0.7715209722518921)\n",
            "Rank 4: User 2 (Similarity Score: 0.7578029632568359)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rank's Output with User1 using using Word2Vec embeddings**"
      ],
      "metadata": {
        "id": "9WSBbVAWDDHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Rank matches with user1 using word2vec embeddings:')\n",
        "for i, ix in enumerate(sorted_indices_word2vec, 1):\n",
        "  print(f'Rank {i}: User {ix +2} (Similarity Score: {bert_similarities[ix]})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KoNNwFXC1_Z",
        "outputId": "a1a3421c-6337-4091-c288-8d112b7e5a8c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank matches with user1 using word2vec embeddings:\n",
            "Rank 1: User 5 (Similarity Score: 0.8226811289787292)\n",
            "Rank 2: User 3 (Similarity Score: 0.7715209722518921)\n",
            "Rank 3: User 2 (Similarity Score: 0.7578029632568359)\n",
            "Rank 4: User 4 (Similarity Score: 0.7725504040718079)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflections and Further Analysis\n",
        "\n",
        "### How could ranked matching be improved?\n",
        "Ranked matching could be improved by considering additional factors beyond textual similarity. Some improvements include:\n",
        "- **Contextual Understanding:** Incorporating techniques to understand the context of statements better, such as analyzing sentiment, topic modeling, or capturing the nuances of language.\n",
        "--**Alternative Algorithms:** While BERT and Word2Vec embeddings are powerful algorithms, exploring simpler algorithms such as TF-IDF or cosine similarity on bag-of-words representations could provide meaningful insights.\n",
        "- **Feedback Mechanism:** Implementing a feedback mechanism where users can provide ratings or feedback on matches, which can be used to refine the matching algorithm over time.\n",
        "- **Enhanced Data Processing:** Implementing basic data preprocessing techniques such as removing stop words, stemming, or lemmatization can help improve the quality of text based data. This can lead to more accurate similarity calculations and better-ranked matches.\n",
        "\n",
        "### Why did you choose the method you used to complete the analysis?\n",
        "I chose to use BERT and Word2Vec embeddings for computing similarity scores because of their effectiveness in capturing semantic relationships between text. BERT embeddings, being contextualized, provide a good representation of the meaning of sentences, while Word2Vec embeddings capture semantic similarity based on word co-occurrences. By utilizing both methods, I can leverage their respective strengths to enhance the accuracy of the matching process.\n",
        "\n",
        "### Other than your chosen method, what other methods would you pursue?\n",
        "In addition to BERT and Word2Vec embeddings, other methods that could be explored for improving the matching process include:\n",
        "- **TF-IDF:** TF-IDF (Term Frequency-Inverse Document Frequency) to represent the importance of words in documents and compute similarity scores based on weighted word frequencies.\n",
        "- **Doc2Vec:** Using Doc2Vec to generate document-level embeddings, which can capture the overall semantic meaning of a document.\n",
        "- **Deep Learning Models:** Exploration of more advanced deep learning models which is designed for similarity matching tasks, such as transformer-based models tailored for similarity computation.\n",
        "\n",
        "Each of these methods has its advantages and can contribute to improving the accuracy and effectiveness of the ranked matching process.\n"
      ],
      "metadata": {
        "id": "9ktFDbamEN8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkhbP56RJTcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}